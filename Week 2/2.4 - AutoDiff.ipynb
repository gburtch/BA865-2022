{"cells":[{"cell_type":"markdown","metadata":{"id":"oV_kcG6Y1T8L"},"source":["#**Automatic Differentiation**"]},{"cell_type":"markdown","metadata":{"id":"3WUDvR031aD_"},"source":["In this notebook, I'm going to demonstrate GradientTape(), a function that implements a computation graph, to automate the evaluation of partial derivatives. Here, we define a function, y = x^2, we define a gradientTape, and then we call the gradient() function to evaluate the first derivative of that function with respect to one parameter of that function, in this case x. Note that a tf.Variable() is a tensor that can hold mutable (changeable) values. Here I have defined x as a scalar. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1642894581380,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"},"user_tz":300},"id":"uo86Hd5d0756","outputId":"41555b01-0f0f-43e1-a9ed-33cd09b76758"},"outputs":[{"name":"stdout","output_type":"stream","text":["The first derivative of x^2 is 6.0\n"]},{"name":"stderr","output_type":"stream","text":["2022-01-25 10:23:48.929521: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape:\n","  y = x**2\n","\n","# f(x) = x^2; f'(x) = 2x; numerical evaluation of 2x when x = 3 is 2*3 = 6.\n","print(f'The first derivative of x^2 is {tape.gradient(y,x).numpy()}')"]},{"cell_type":"markdown","metadata":{"id":"u0E6-nJ9JVkE"},"source":["We can also nest tapes to take a second derivative, like so:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1642894686265,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"},"user_tz":300},"id":"UhBUhCK3JY6f","outputId":"ecd00737-56d2-4542-cf38-c0b1beb21974"},"outputs":[{"name":"stdout","output_type":"stream","text":["The first derivative of x^2 is 2x; 2 * 3 = 6.0\n","The second derivative of x^2 is 2.0\n"]}],"source":["with tf.GradientTape() as tape2:\n","  # If we don't specify 'persistent = True' then the tape is dropped from memory after the first gradient is evaluated.\n","  with tf.GradientTape(persistent=True) as tape:\n","     y=x**2\n","\n","  dy_dx = tape.gradient(y,x)\n","  print(f'The first derivative of x^2 is 2x; 2 * 3 = {dy_dx.numpy()}')\n","\n","d2y_dx = tape2.gradient(dy_dx,x)\n","\n","print(f'The second derivative of x^2 is {d2y_dx.numpy()}')"]},{"cell_type":"markdown","metadata":{"id":"O0Rx9a5M3iK5"},"source":["We can also use tapes with higher dimensional tensors, e.g., here is an example for a sigmoid neuron. In the code below, note that w@x is tensor multiplication. Be careful with the order of the matrices and their dimensions; the matrix multiplication will only work if the shapes align. Also note that reduce_mean() returns the average over elements of y. We can directly obtain the gradient of the loss function w.r.t. our w and b parameters. The resulting gradients will be the same shape as the argument we are taking gradient with respect to, i.e., w.shape and b.shape. So, when we update values in a back pass, we can just calculate w -= dl_dw * learning_rate. and b -= dl_db * learning_rate. "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160,"status":"ok","timestamp":1642895030890,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"},"user_tz":300},"id":"g2gu5SDC3mCn","outputId":"430d917c-4423-46b6-9ecc-ace33fb05f2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["The shape of x*w+b will be: (1, 2)\n","\n","The gradient of loss w.r.t. w is\n","[[1.1713153e-01 2.9073123e-04]\n"," [2.3426306e-01 5.8146246e-04]\n"," [5.8565766e-01 1.4536561e-03]]\n"," The gradient w.r.t. b is [0.11713153 0.00029073].\n"]}],"source":["w = tf.Variable(tf.random.normal((3,2),dtype=tf.float32),name='w')\n","b = tf.Variable(tf.ones(2,dtype=tf.float32),name='b')\n","x = tf.constant([[1., 2., 5.]])\n","\n","# x is a 1x3 vector; w is a 3x2 matrix. So, 1x3 * 3*2 = 1x2 result, and adding a 1x2 vector gives coherent output of the same shape. \n","print(f\"The shape of x*w+b will be: {(x@w+b).numpy().shape}\\n\")\n","\n","with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  z = tf.sigmoid(y)\n","  loss = tf.reduce_mean(z)\n","\n","[dl_dw,dl_db] = tape.gradient(loss,[w,b])\n","print(f'The gradient of loss w.r.t. w is\\n{dl_dw}\\n The gradient w.r.t. b is {dl_db}.')"]},{"cell_type":"markdown","metadata":{"id":"yUHzDpje-p-J"},"source":["Let's do the same thing, but using a dictionary instead of a list..."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1642442499228,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"},"user_tz":300},"id":"awtrxduf-uhf","outputId":"0cab0e2e-b991-4bb7-d0ec-31e1ba311d6e"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[0.5, 0.5],\n","       [1. , 1. ],\n","       [2.5, 2.5]], dtype=float32)>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["with tf.GradientTape() as tape:\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y)\n","\n","my_parms = { \n","    'w': w,\n","    'b': b\n","}\n","\n","gradients = tape.gradient(loss,my_parms)\n","gradients['w']"]},{"cell_type":"markdown","metadata":{"id":"WBOW960YYoDp"},"source":["#**Gradient Descent with a Simple Neural Network**"]},{"cell_type":"markdown","metadata":{"id":"U-mXbcgs_0Zy"},"source":["Okay, now let's apply GradientTape to a mock neural network. We have our input layer of x's (4 features), and then a 3-node hidden layer employing ReLU activations, followed by a 4-node hidden layer employing tanh activations, followed by a single output node, employing sigmoid, for a binary DV. Our loss function will be the average of squared predictions. Note that this isn't really a meaningful NN; it's just for show. This definition means we have 3*2 = 6 weights and 2 bias terms. "]},{"cell_type":"code","execution_count":26,"metadata":{"id":"8rr-fFhN_4dH"},"outputs":[],"source":["# Define our learning rate and our ground truth value for our single training example.\n","learning_rate = 1e-3\n","target=15\n","\n","# Dense implements the operation: output = activation(dot(input, weights) + bias).\n","# Units = 2 means we have two nodes in the layer.\n","# We have 4 inputs, a hidden layer with 2 nodes, thus 4x2 = 8 weights, randomly initialized;\n","# The vector of 2 bias terms will initially default to values of 0.\n","# We then have a sigmoid output layer, e.g., a binary classification scenario, which takes 2 inputs, thus 2*1 = 2 weights, 1 bias term.\n","model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=2, activation=\"relu\", name=\"hiddenLayer\"),\n","        tf.keras.layers.Dense(units=1, activation=\"linear\",name=\"outputLayer\"),\n","])\n","\n","# Here is a random vector of 4 values, e.g., representing a single training observation.\n","input = tf.random.normal((1,4))"]},{"cell_type":"markdown","metadata":{"id":"gyUju8YLMjH0"},"source":["Now we can apply GradientTape, in a loop, conducting a forward pass and then recovering gradients for all model parameters, and finally updating our parameters in the opposite direction from the gradients. We will iterate and repeat this process 100 times. Note, if we were doing this with N training examples, we would instead take the average loss across them. So, loss would equal tf.reduce_mean((prediction-target)**2)). We are sticking with a single training data point here for simplicity's sake."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"UrbO4KN-MjkW"},"outputs":[],"source":["import numpy as np\n","\n","history = []\n","for i in range(2000):\n","  with tf.GradientTape() as tape:\n","    # Forward pass\n","    prediction = model(input)\n","    # We define our loss as the square of the forward pass prediction - 3. \n","    # Three is the true value, and we are taking the squared loss with respect to that value.\n","    loss = (prediction-target)**2\n","\n","  # Gradients with respect to every trainable variable, i.e., backward pass.\n","  grad = tape.gradient(loss, model.trainable_variables)\n","  for i in range(len(model.trainable_variables)):\n","    new_parms = model.trainable_variables[i] - grad[i]*learning_rate\n","    model.trainable_variables[i].assign(new_parms)\n","  \n","  history.append(prediction.numpy())\n","\n","# Finally, collapse the list of arrays into a single array.\n","history = np.concatenate(history, axis=0)"]},{"cell_type":"markdown","metadata":{"id":"ZNGYTEtzThLp"},"source":["Finally, we can plot the optimization process, showing how our model gradually improves until it predicts the correct value."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1642446077472,"user":{"displayName":"Gordon Burtch","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6kdrNKuddVmCp6HcajLgk8KM0o5MC7oJKYfMbVGU=s64","userId":"10144756805379529333"},"user_tz":300},"id":"UEERAnfWTlX-","outputId":"b8a06cb7-2d27-4371-be90-0d16bf68a4f2"},"outputs":[{"data":{"text/plain":["Text(0.5, 1.0, 'Optimization of a Simple NN')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Plot the evolution of our predictions as we tweak our network's weights and bias.\n","# Eventually, it converges to the correct prediction.\n","plt.plot(range(2000),history,[target]*2000)\n","plt.legend(('Predicted','True'))\n","plt.xlabel('Iteration')\n","plt.ylabel('y value')\n","plt.title('Optimization of a Simple NN')"]},{"cell_type":"markdown","metadata":{"id":"1fnbHC20LxJ2"},"source":["#**Gradient Tape Exercise**"]},{"cell_type":"markdown","metadata":{"id":"69fQkLUSL4Ha"},"source":["Use tf.GradientTape() to plot the Hyperbolic Tangent activation function over input values ranging from -10 to 10, and overlay a plot of the first derivative of the activation function. \n","\n","Notes:\n","\n","* *Any variable you want to calculate a derivative with respect to should be declared as a tf.Variable().*\n","* *To convert a tf tensor into a numpy array, apply the .numpy() function to it.* \n","* *The tanh function is pre-defined for you in tf.nn.tanh(), as are some other activation functions.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBo6mNhjMDD1"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1508104464.py, line 8)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"/var/folders/yf/5y3529gn3vdgd3wc6l8vvxgr0000gn/T/ipykernel_43932/1508104464.py\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    with # Declare your tape here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["import numpy as np\n","\n","# Simulating some data for you to initialize x with. This is just a 200x1 array of random values between -10 and +10, sorted.\n","data = tf.sort(tf.random.uniform((200,1),minval=-10,maxval=10),axis=0)\n","\n","x = 5 ## Declare your input variable.\n","\n","with tf.GradientTape() as tape:# Declare your tape here\n","  # enter the activation function here\n","      # Forward pass\n","    prediction = model(x)\n","    # We define our loss as the square of the forward pass prediction - 3. \n","    # Three is the true value, and we are taking the squared loss with respect to that value.\n","    loss = (prediction-target)**2\n","\n","dy_dx = # Obtain the gradient here. \n","\n","# Create your plot\n","plt.plot() # argumenets here will be your x and y for the activation function. \n","plt.plot() # arguments here are your x and y' for the activation function\n","plt.legend(['Tanh','Tanh\\''])\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPNbF9JPOZkoIVvJ2PkMz70","collapsed_sections":[],"name":"2.4 - AutoDiff.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}
